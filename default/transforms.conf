# The Metrics log lines use a comma to break up events. This first transform removes the preamble (date, log level, component etc)
# and converts it not our format for popping and cloning
# 9/11/17 8:47:13.635 PM 09-11-2017 20:47:13.635 +0100 INFO  Metrics - group=thruput, name=thruput
[build_metric_names_metrics]
REGEX= (?:INFO  Metrics - group=([a-zA-Z_\-]+))(?:, )?(.*)
FORMAT = metrics.$1#$2
DEST_KEY = _raw

# The License log lines use a space to break up events. This first transform removes the preamble (date, log level, component etc)
# and converts it not our format for popping and cloning
# 09-11-2017 20:34:42.653 +0100 INFO  LicenseUsage - type=Usage s=looper st=splunk_metric h="rmorgan-mbp" o="" idx="main" i="2ECD2EBB-03A2-4C2A-AFD3-BBEE939E99D0" pool="auto_generated_pool_download-trial" b=49535 poolsz=524288000
[build_metric_names_license_usage]
REGEX = (?:INFO  LicenseUsage - )(.*)$
FORMAT = license_usage#$1
DEST_KEY = _raw

[build_metric_names_scheduler]
#09-21-2017 15:03:32.697 +0100 INFO  SavedSplunker - savedsearch_id="nobody;Splunk_CiscoSecuritySuite;_ACCELERATE_48C9DF7F-607E-4A10-B220-53D2B4EF8CE9_Splunk_CiscoSecuritySuite_nobody_e800ae558177e730_ACCELERATE_", search_type="report_acceleration", user="nobody", app="Splunk_CiscoSecuritySuite", savedsearch_name="_ACCELERATE_48C9DF7F-607E-4A10-B220-53D2B4EF8CE9_Splunk_CiscoSecuritySuite_nobody_e800ae558177e730_ACCELERATE_", priority=default, status=skipped, reason="The maximum number of concurrent auto-summarization searches on this instance has been reached", concurrency_category="summarization_scheduled", concurrency_context="instance-wide", concurrency_limit=2, scheduled_time=1506002400, window_time=0
REGEX = (?:INFO  SavedSplunker - )(.*)$
FORMAT = scheduler#$1
DEST_KEY = _raw

# Any key=value pair where the "value" is not a number we want to add this to the _meta
# so that they will become dimensions for how the data can be cut
[extract_dimensions]
REGEX = \s([a-zA-Z_\-]+)=("[^"]+"|[a-zA-Z_/][a-zA-Z0-9_/\-\.\-]*)
WRITE_META = true
REPEAT_MATCH = true
FORMAT = $1::$2

# This is like 'normal' dimensions but customised to remove the sid attribute as that is unique per 
# execution and is unnecessarily bloating the tsidx 
[extract_dimensions_scheduler]
REGEX = \s([a-zA-Z_\-]+(?<!sid))=("[^"]+"|[a-zA-Z_/][a-zA-Z0-9_/\-\.\-]*)
WRITE_META = true
REPEAT_MATCH = true
FORMAT = $1::$2

# This popping transform assumes that the delim is a comma
# When we can pop no more we are "finished"
[pop_with_comma]
REGEX = ^([^#]+)#(?:(?:[a-zA-Z_\-\.*]+)=(?:"[^"]+"|[^\s,]+)|[^,]+), (.+)$
FORMAT = $1#$2
DEFAULT_VALUE = finished
DEST_KEY = _raw

# This popping transform assumes that the delim is a space
# When we can pop no more we are "finished"
[pop_with_space]
REGEX = ^([^#]+)#(?:[a-zA-Z_\-\.]+)=(?:"[^"]+"|[^\s]+)\s(.+)$
FORMAT = $1#$2
DEFAULT_VALUE = finished
DEST_KEY = _raw

# This transform performs the actual clone, it does not modify the event in any way.
# If this only cloned when the regex matched, it would reduce the number of unnecessary clones
[clone]
REGEX = .
FORMAT = $0
DEST_KEY = _raw
CLONE_SOURCETYPE = splunk_metric

[clone_splunkd]
REGEX = .
FORMAT = $0
DEST_KEY = _raw
CLONE_SOURCETYPE = splunkd

# We need to change the sourcetype as it is too confusing to allow the clones to hit the same stanza
# The new source is not used for any logic
[change_source_license]
REGEX = .
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/splunk/license.log

[change_source_web_service]
REGEX = .
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/splunk/web_service.log

# We need to change the sourcetype as it is too confusing to allow the clones to hit the same stanza
# The new source is not used for any logic
[change_source_metrics]
REGEX = (?:.*)
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/splunk/metrics.log

[change_source_scheduler]
REGEX = (?:.*)
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/splunk/scheduler.log

# The last function to call, this parses the current metric and assigns its value to value
# and builds its metric name. Attribute is added to help search. After this, teh event is 
# ready for the metrics index
[make_metric]
REGEX = ^([^#]+)#([^=,]+)=("[^"]+"|[^, ]+),?
WRITE_META = true
FORMAT = metric_name::$1.$2 _value::$3 attribute::$2

# Elements that have no more variables to 'pop' set themselves as "finished", we have 
# to intercept these and route to null
[drop-finished]
SOURCE_KEY = _raw
REGEX = ^finished$
DEST_KEY = queue
FORMAT = nullQueue

# All events with _values that are just string should be routed to null, anything else should 
[reroute-numbers]
SOURCE_KEY = _meta
REGEX = _value::\d*(?:.\d*)?\s?
DEST_KEY = _MetaData:Index
FORMAT = metrics_internal

# All events with _values that are just string should be routed to null, anything else should 
[drop-strings]
SOURCE_KEY = _meta
REGEX = _value::(?:[0..9]*/D+[0..9]*)\s?
DEST_KEY = queue
FORMAT = nullQueue

# By default we send all data to the metrics_internal_detritus, anything that can be 
# successfully extracted as a number will be will be routed to a real metrics store
[set_index_metrics]
REGEX = .
DEST_KEY = _MetaData:Index
FORMAT = metrics_internal_detritus

[set_st_splunk_metric]
REGEX = .
DEST_KEY = MetaData:Sourcetype
FORMAT = sourcetype::splunk_metric

# All the transform to get the web log performance data

[clone_web_view_time]
REGEX = (never match)
DEST_KEY = _raw
CLONE_SOURCETYPE = splunk_metrics_web_view_time

[clone_web_template_time]
REGEX = (never match)
DEST_KEY = _raw
CLONE_SOURCETYPE = splunk_metrics_web_template_time

# we only need some of the web log stream, so route everything to null, and then we can pick out what we want to keep
[route_to_null]
REGEX = .
DEST_KEY = queue
FORMAT = nullQueue

# if the regex matches then pull the event back from going into the null queue
[save_perf]
REGEX = INFO\s+\[(?:[a-f0-9]+)\]\s+view:\d+\s+-\s+PERF
DEST_KEY = queue
FORMAT = indexQueue

[extract_web_perf_view_time]
#REGEX =INFO\s+\[(?<client_id>[a-f0-9]+)\] view:(?<view_id>\d+)\s+-\s+PERF\s-\s+viewType=(?<viewType>[a-z]+)\sviewTime=(?<view_time>\d+|\d+\.\d+)s\stemplateTime=(?<templateTime>\d+|\d+\.\d+)s
REGEX =INFO\s+\[([a-f0-9]+)\] view:(\d+)\s+-\s+PERF\s-\s+viewType=([a-z]+)\sviewTime=(\d+|\d+\.\d+)s\stemplateTime=(?:\d+|\d+\.\d+)s
WRITE_META = true
FORMAT = metric_name::web_ui.view_time client_id::$1 view_id::$2 view_type::$3 _value::$4

[extract_web_perf_template_time]
#REGEX =INFO\s+\[(?<client_id>[a-f0-9]+)\] view:(?<view_id>\d+)\s+-\s+PERF\s-\s+viewType=(?<viewType>[a-z]+)\sviewTime=(?<view_time>\d+|\d+\.\d+)s\stemplateTime=(?<templateTime>\d+|\d+\.\d+)s
REGEX =INFO\s+\[([a-f0-9]+)\] view:(\d+)\s+-\s+PERF\s-\s+viewType=([a-z]+)\sviewTime=(?:\d+|\d+\.\d+)s\stemplateTime=(\d+|\d+\.\d+)s
WRITE_META = true
FORMAT = metric_name::web_ui.template_time client_id::$1 view_id::$2 view_type::$3 _value::$4

# the following section is for processing the json events from disk objects

[clone_splunk_disk_objects]
REGEX = .
DEST_KEY = _raw
FORMAT = $0
CLONE_SOURCETYPE = splunk_disk_objects

[copy_meta_to_raw]
REGEX = ^(.*)$
SOURCE_KEY = _meta
DEST_KEY = _raw
FORMAT = wtf $1

[clone_http_event_collector]
REGEX = .
DEST_KEY = _raw
FORMAT = $0
CLONE_SOURCETYPE = http_event_collector_metrics

[change_source_http_event_collector]
REGEX = .
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/introspection/http_event_collector_metrics.log

[change_source_disk_objects]
REGEX = .
SOURCE_KEY = MetaData:Source
DEST_KEY = MetaData:Source
FORMAT = source::.../log/introspection/disk_objects.log

# we want to remove all the json indexed field stuff from _meta as it has been copied into 
# _raw, but we want to retain the time information. Also lets drop the datetime extraction.
# datetime::"09-28-2017 12:55:25.112 +0100" log_level::INFO component::HttpEventCollector data.series::http_event_collector data.transport::http data.format::json data.total_bytes_received::0 data.total_bytes_indexed::0 data.num_of_requests::0 data.num_of_events::0 data.num_of_errors::0 data.num_of_parser_errors::0 data.num_of_auth_failures::0 data.num_of_requests_to_disabled_token::0 data.num_of_requests_to_incorrect_url::0 data.num_of_requests_in_mint_format::0 data.num_of_ack_requests::0 data.num_of_requests_acked::0 data.num_of_requests_waiting_ack::0
[clear_json_from_meta]
REGEX = ^datetime::"[^"]+"\slog_level::[A-Z]+\s.*\s(_subsecond::.*)$
FORMAT = $1
SOURCE_KEY = _meta
DEST_KEY = _meta

# these are the auto indexed time values, we want to keep these as useful dimensions, also _subsecond value is required for subsecond counting
# _subsecond::.953 date_second::51 date_hour::10 date_minute::17 date_year::2017 date_month::september date_mday::28 date_wday::thursday date_zone::60
[copy_json_meta_to_raw]
REGEX = ^datetime::"[^"]+"\slog_level::[A-Z]+\s(.*)\s_subsecond::.*$
FORMAT = $1
SOURCE_KEY = _meta
DEST_KEY = _raw

[build_metric_names_disk_objects]
# datetime::"09-22-2017 16:59:37.127 +0100" log_level::INFO component::Partitions data.capacity::475946.125 data.fs_type::hpfs data.mount_point::/ data.available::82312.117 data.free::82562.117 timestartpos::13 timeendpos::42 _subsecond::.127 date_second::37 date_hour::16 date_minute::59 date_year::2017 date_month::september date_mday::22 date_wday::friday date_zone::60
REGEX = component::([a-z|A-Z]+)\s+(.*)$
FORMAT = disk_objects.$1#$2
DEST_KEY = _raw

[build_metric_names_http]
# datetime::"09-22-2017 16:59:37.127 +0100" log_level::INFO component::Partitions data.capacity::475946.125 data.fs_type::hpfs data.mount_point::/ data.available::82312.117 data.free::82562.117 timestartpos::13 timeendpos::42 _subsecond::.127 date_second::37 date_hour::16 date_minute::59 date_year::2017 date_month::september date_mday::22 date_wday::friday date_zone::60
REGEX = ^(.*)$
FORMAT = http_event_collector#$1
DEST_KEY = _raw

[extract_dimensions_json]
REGEX = ((:?[a-zA-Z_\-]+)::(?:"[^"]+"|[a-zA-Z_/][a-zA-Z0-9_/\-\.\-]*))
WRITE_META = true
REPEAT_MATCH = true
FORMAT = $1

[change_st_temporary]
REGEX = .
DEST_KEY = MetaData:Sourcetype
FORMAT = sourcetype::temporary_sourcetype

[set_st_json_metrics_extraction]
REGEX = .
DEST_KEY = MetaData:Sourcetype
FORMAT = sourcetype::json_metrics_extraction

[clone_json_http_event_collector]
REGEX = .
FORMAT = $0
DEST_KEY = _raw
CLONE_SOURCETYPE = http_event_collector_metrics

[clone_json_splunk_disk_objects]
REGEX = .
FORMAT = $0
DEST_KEY = _raw
CLONE_SOURCETYPE = splunk_disk_objects

[clone_json]
REGEX = .
FORMAT = $0
DEST_KEY = _raw
CLONE_SOURCETYPE = json_metrics_extraction

[pop_json]
REGEX = ^([^#]+)#(?:[a-zA-Z_\-\.*]+)::(?:"(?:[^"]+)"|[^\s]+)\s+(.+)$
FORMAT = $1#$2
DEFAULT_VALUE = finished
DEST_KEY = _raw

[make_json_metric]
REGEX = ^([^#]+)#([^:]+)::("[^"]+"|[^ ]+)\s?
WRITE_META = true
FORMAT = metric_name::$1.$2 _value::$3 attribute::$2

# unfortunately I cannot get regex to discard the "top_apps" "top_user" etc from disk_objects
# which is polluting my metric names with user and app names, lets route them to null to stop indexing them.
[drop_top_apps_etc]
REGEX = metric_name::"?disk_objects\.Dispatch\.data.top_(?:apps|user|named_searches)
SOURCE_KEY = _meta
DEST_KEY = queue
FORMAT = nullQueue
